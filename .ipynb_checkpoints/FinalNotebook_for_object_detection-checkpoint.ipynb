{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DEEP LEARNING BASED FOOD CALORIE ESTIMATOR\n",
    "\n",
    "### WHAT AM I EATING?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training model for binary classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import tensorflow as tf\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Activation, Dropout, Flatten, Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = Sequential()    #Creating an object of the sequential class below\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.add(Conv2D(32, (3, 3), padding='same', input_shape = (100, 100, 3), activation = 'relu'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.add(Conv2D(32, (3, 3), activation='relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "classifier.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.add(Dropout(0.25))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Adding additional bundles of layers similar to above will help our model to detect more complex features which will improve the classification accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.add(Conv2D(64, (3, 3), padding='same', activation = 'relu'))\n",
    "classifier.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "classifier.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "classifier.add(Dropout(0.25))\n",
    "\n",
    "classifier.add(Conv2D(64, (3, 3), padding='same', activation = 'relu'))\n",
    "classifier.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "classifier.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "classifier.add(Dropout(0.25))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Flatten will flatten the feature map to one dimension\n",
    "* Then we add a fully connected layer which will convert our feature map to n-dimentional vector\n",
    "* Then we apply our final dropout layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.add(Flatten())\n",
    "classifier.add(Dense(units = 512, activation = 'relu'))\n",
    "classifier.add(Dropout(0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Dense is the function to add a fully connected layer, ‘units’ is where we define the number of nodes that should be present in this hidden layer, these units value will be always between the number of input nodes and the output nodes\n",
    "* In the final dense(fully-connected) layer, we convert the vector map into the vector map with number of vectors equal to the number of classes our model needs to classify.\n",
    "* The activation function for last layer of the model will convert the vector map into the probability for the number of classes specified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.add(Dense(2, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Now we compile with this model with a loss function which measures difference between the predictions and expectations\n",
    "* This loss propogates backwards (taking derivative of loss with respect to weights) updating weight values so that they change in the direction of the gradient that will minimize the loss in future.\n",
    "* The optimizer specified will perform gradient descend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.compile(optimizer = 'rmsprop', loss = 'binary_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Then we do image preprocessing so that all the images we provide to model in the training phase would be of size 100x100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(rescale = 1./255, shear_range = 0.2, zoom_range = 0.2, horizontal_flip = True)\n",
    "test_datagen = ImageDataGenerator(rescale = 1./255)\n",
    "training_set = train_datagen.flow_from_directory('Fruit_360/2/Training',\n",
    "                                                 target_size = (100, 100),\n",
    "                                                 batch_size = 32,\n",
    "                                                 class_mode = 'categorical')\n",
    "test_set = test_datagen.flow_from_directory('Fruit_360/2/Testing',\n",
    "                                            target_size = (100, 100), \n",
    "                                            batch_size = 32, \n",
    "                                            class_mode = 'categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.fit_generator(training_set,\n",
    "steps_per_epoch = 982/32,\n",
    "epochs = 5,\n",
    "validation_data = test_set,\n",
    "validation_steps = 330/32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Function to identify the type of fruit and its count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from IPython.display import Image\n",
    "from keras.preprocessing import image\n",
    "\n",
    "imageID = 1\n",
    "\n",
    "## Initial count of apples and banana set to 0\n",
    "AppleCount=0\n",
    "BananaCount=0\n",
    "for i in range(10):\n",
    "    ## Reading each image in path\n",
    "    path = \"Fruit_360/2/val/\" + str(imageID) + \".jpg\"\n",
    "    imageID+=1\n",
    "    test_image = image.load_img(path, target_size = (100, 100))\n",
    "    test_image = image.img_to_array(test_image)\n",
    "    test_image = np.expand_dims(test_image, axis = 0)\n",
    "    result = classifier.predict(test_image)\n",
    "    training_set.class_indices\n",
    "    \n",
    "    ## if 0 then apple predicted, else banana\n",
    "    if result[0][0] == 1:\n",
    "        display(Image(filename = path, width=100, height=100))\n",
    "        prediction = 'Apple'\n",
    "        print(prediction)\n",
    "        AppleCount+=1\n",
    "        \n",
    "    elif result[0][0] == 0:\n",
    "        display(Image(filename = path, width=100, height=100))\n",
    "        prediction = 'Banana'\n",
    "        print(prediction)\n",
    "        BananaCount+=1\n",
    "\n",
    "print(\"Number of Apples predicted: \", AppleCount)\n",
    "print(\"Number of Bananas predicted: \", BananaCount)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training for multiple classification with change in parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Linear stack of layers\n",
    "classifier3 = Sequential()   \n",
    "classifier3.add(Conv2D(32, (3, 3), padding='same', input_shape = (100, 100, 3), activation = 'relu'))\n",
    "classifier3.add(Conv2D(32, (3, 3), activation='relu'))\n",
    "classifier3.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "classifier3.add(Dropout(0.25))\n",
    "\n",
    "classifier3.add(Conv2D(64, (3, 3), padding='same', activation = 'relu'))\n",
    "classifier3.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "classifier3.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "classifier3.add(Dropout(0.25))\n",
    "\n",
    "classifier3.add(Conv2D(64, (3, 3), padding='same', activation = 'relu'))\n",
    "classifier3.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "classifier3.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "classifier3.add(Dropout(0.25))\n",
    "\n",
    "classifier3.add(Flatten())\n",
    "classifier3.add(Dense(units = 512, activation = 'relu'))\n",
    "classifier3.add(Dropout(0.5))\n",
    "classifier3.add(Dense(60, activation='softmax'))\n",
    "classifier3.compile(optimizer = 'rmsprop', loss = 'categorical_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(rescale = 1./255, shear_range = 0.2, zoom_range = 0.2, horizontal_flip = True)\n",
    "test_datagen = ImageDataGenerator(rescale = 1./255)\n",
    "training_set = train_datagen.flow_from_directory('Fruit_360/60/Training',\n",
    "                                                 target_size = (100, 100),\n",
    "                                                 batch_size = 32,\n",
    "                                                 class_mode = 'categorical')\n",
    "test_set = test_datagen.flow_from_directory('Fruit_360/60/Testing',\n",
    "                                            target_size = (100, 100), \n",
    "                                            batch_size = 32, \n",
    "                                            class_mode = 'categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier3.fit_generator(training_set,\n",
    "steps_per_epoch = 28736/32,\n",
    "epochs = 10,\n",
    "validation_data = test_set,\n",
    "validation_steps = 9673/32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (classifier.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (classifier3.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "# Dependencies for Object Detection:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To successfully perform the object detection, we first need to install tensorflow for our python environment.\n",
    "\n",
    "*Tensorflow Object Detection API depends on the following libraries:*\n",
    "\n",
    "* Protobuf 2.\n",
    "* Python-tk\n",
    "* Pillow 1.0\n",
    "* lxml\n",
    "* tf Slim (which is included in the \"tensorflow/models/research/\" checkout)\n",
    "* Jupyter notebook\n",
    "* Keras\n",
    "* Matplotlib\n",
    "* Tensorflow\n",
    "* Cython\n",
    "* cocoapi\n",
    "* ssd\n",
    "* Pandas\n",
    "\n",
    "To install tensorflow:\n",
    "\n",
    "* For CPU\n",
    "\n",
    "pip install tensorflow\n",
    "\n",
    "* For GPU\n",
    "\n",
    "pip install tensorflow-gpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Dataset used is scrapped image dataset from internet for object detection. The dataset contains images belonging to 10 different classes. These images are all labeled with all the labels stored in corrosponding .xml file for each image file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Protobuf Compilation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The Tensorflow Object Detection API uses Protobufs to configure model and training parameters. Before the framework can be used, the Protobuf libraries must be compiled. This should be done by running the following command from the tensorflow/models/research/ directory:\n",
    "\n",
    "        #From tensorflow/models/research/\n",
    "    \n",
    "        protoc object_detection/protos/*.proto --python_out=."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add Libraries to PYTHONPATH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* When running locally, the tensorflow/models/research/ and slim directories should be appended to PYTHONPATH. This can be done by running the following from tensorflow/models/research/:\n",
    "\n",
    "        #From tensorflow/models/research/\n",
    "        \n",
    "        export PYTHONPATH=$PYTHONPATH:`pwd`:`pwd`/slim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Testing the Installation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* You can test that you have correctly installed the Tensorflow Object Detection\n",
    "API by running the following command:\n",
    "        \n",
    "        python object_detection/builders/model_builder_test.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuring the Object Detection Training Pipeline "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The Tensorflow Object Detection API uses protobuf files to configure the training and evaluation process. The schema for the training pipeline can be found in object_detection/protos/pipeline.proto.\n",
    "\n",
    "        A skeleton configuration file is shown below:\n",
    "        \n",
    "        model {\n",
    "                (... Add model config here...)\n",
    "              }\n",
    "\n",
    "        train_config :\n",
    "              {\n",
    "                (... Add train_config here...)\n",
    "              }\n",
    "\n",
    "        train_input_reader: \n",
    "              {\n",
    "                (... Add train_input configuration here...)\n",
    "              }\n",
    "\n",
    "        eval_config: \n",
    "              {\n",
    "                  \n",
    "              }\n",
    "\n",
    "       eval_input_reader: \n",
    "             {\n",
    "                (... Add eval_input configuration here...)\n",
    "             }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining Inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The Tensorflow Object Detection API accepts inputs in the TFRecord file format. Users must specify the locations of both the training and evaluation files. Additionally, label map should be specified , which define the mapping between a class id and class name. The label map should be identical between training and evaluation datasets.\n",
    "\n",
    "        An example input configuration looks as follows:\n",
    "        \n",
    "        tf_record_input_reader {\n",
    "                                  input_path: \"/usr/home/username/data/train.record\"\n",
    "                               }\n",
    "        label_map_path: \"/usr/home/username/data/label_map.pbtxt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing Inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Tensorflow Object Detection API reads data using the TFRecord file format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CODE TO CHECK IF IMAGE HAS MORE THAN RGB COMPONENT\n",
    "from PIL import Image     \n",
    "import os       \n",
    "path = 'images/' \n",
    "for file in os.listdir(path):      \n",
    "     extension = file.split('.')[-1]\n",
    "     if extension == 'jpg' or extension == 'jpeg' or extension == 'png':\n",
    "           fileLoc = path+file\n",
    "           img = Image.open(fileLoc)\n",
    "           if img.mode != 'RGB':\n",
    "                 print(file+', '+img.mode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Deleted such images because tensors are generated of more size than expected and so it crashes, to ensure this does not happen, we deleted such images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating the TFRecord files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Images are labelled using lblImg [https://github.com/tzutalin/labelImg] which is in xml format. \n",
    "* XML files are then converted to csv format using sml_to_csv.py [https://github.com/datitran/raccoon_dataset/blob/master/xml_to_csv.py]\n",
    "\n",
    "        Now the folder structure for object detection will look as follows\n",
    "        \n",
    "        Object-Detection\n",
    "        -data/\n",
    "            --test_labels.csv\n",
    "            --train_labels.csv\n",
    "        -images/\n",
    "            --test/\n",
    "                ---testingimages.jpg\n",
    "            --train/\n",
    "                ---testingimages.jpg\n",
    "            --...yourimages.jpg\n",
    "        -training\n",
    "        -xml_to_csv.py\n",
    "        \n",
    "* To convert these into TFRecords, run the following commands:\n",
    "\n",
    "        python generate_tfrecord.py --csv_input=data/train_labels.csv --output_path=data/train.record\n",
    "\n",
    "        python generate_tfrecord.py --csv_input=data/test_labels.csv --output_path=data/test.record"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow Object Detection Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* model used for object detection is coco trained ssd_mobilenet_v1 which can be downloaded from [https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md] with the configuration file ssd_mobilenet_v1_pets.config from [https://github.com/tensorflow/models/tree/master/research/object_detection/samples/configs]\n",
    "\n",
    "* start the training with:\n",
    "\n",
    "        python train.py --logtostderr --train_dir=training/ --pipeline_config_path=training/ssd_mobilenet_v1_pets.config\n",
    "        \n",
    "* From models/object_detection, via terminal, start TensorBoard with:\n",
    "\n",
    "        tensorboard --logdir='training'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the inference graph from the trained model checkpoint file\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "        python export_inference_graph.py --input_type image_tensor --pipeline_config_path training/ssd_mobilenet_v1_pets.config --trained_checkpoint_prefix training/model.ckpt-200 --output_directory 8fruit_graph\n",
    "        \n",
    "* This inference graph can be used to test out the object detection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "# Object Detection Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-22 16:53:10.613045: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import six.moves.urllib as urllib\n",
    "import sys\n",
    "import tarfile\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "\n",
    "from collections import defaultdict\n",
    "from io import StringIO\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "# This is needed since the notebook is stored in the object_detection folder.\n",
    "sys.path.append(\"..\")\n",
    "from object_detection.utils import ops as utils_ops\n",
    "\n",
    "if tf.__version__ < '1.4.0':\n",
    "  raise ImportError('Please upgrade your tensorflow installation to v1.4.* or later!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is needed to display the images.\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Object detection imports\n",
    "Here are the imports from the object detection module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'label_map_util' from 'utils' (/Users/sunil.kashyap/sunil/krishna/Food-calorie-estimation-using-deep-learning/Final Project/env/lib/python3.9/site-packages/utils/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m label_map_util\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m visualization_utils \u001b[38;5;28;01mas\u001b[39;00m vis_util\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'label_map_util' from 'utils' (/Users/sunil.kashyap/sunil/krishna/Food-calorie-estimation-using-deep-learning/Final Project/env/lib/python3.9/site-packages/utils/__init__.py)"
     ]
    }
   ],
   "source": [
    "from utils import label_map_util\n",
    "\n",
    "from utils import visualization_utils as vis_util"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variables\n",
    "\n",
    "Any model exported using the `export_inference_graph.py` tool can be loaded here simply by changing `PATH_TO_CKPT` to point to a new .pb file.  \n",
    "\n",
    "By default we use an \"SSD with Mobilenet\" model here. See the [detection model zoo](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md) for a list of other models that can be run out-of-the-box with varying speeds and accuracies.\n",
    "\n",
    "SSD model provides great speed compared to models like RCNN and RFCN but these has a tradeoff between speed and accuracy.\n",
    "\n",
    "SSD gains speed by doing 2 things in a single step namely : Region proposal network and usage of them in either fully-connected layers or position-sensitive convolutional layers to classify those regions[1]\n",
    "\n",
    "![title](picture.png)\n",
    "\n",
    "\n",
    "Concretely, given an input image and a set of ground truth labels, SSD does the following:\n",
    "\n",
    "1. Pass the image through a series of convolutional layers, yielding several sets of feature maps at different scales (e.g. 10x10, then 6x6, then 3x3, etc.)\n",
    "2. For each location in each of these feature maps, use a 3x3 convolutional filter to evaluate a small set of default bounding boxes. These default bounding boxes are essentially equivalent to Faster R-CNN’s anchor boxes.\n",
    "3. For each box, simultaneously predict a) the bounding box offset and b) the class probabilities\n",
    "4. During training, match the ground truth box with these predicted boxes based on IoU. The best predicted box will be labeled a “positive,” along with all other boxes that have an IoU with the truth >0.5. [1]\n",
    "\n",
    "For more details about the SSD Architecture refer to this link:\n",
    "[1] https://towardsdatascience.com/deep-learning-for-object-detection-a-comprehensive-review-73930816d8d9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What model to download.\n",
    "MODEL_NAME = '8fruit_graph_1'\n",
    "\n",
    "# Path to frozen detection graph. This is the actual model that is used for the object detection.\n",
    "PATH_TO_CKPT = MODEL_NAME + '/frozen_inference_graph.pb'\n",
    "\n",
    "# List of the strings that is used to add correct label for each box.\n",
    "PATH_TO_LABELS = os.path.join('training', 'object-detection.pbtxt')\n",
    "\n",
    "NUM_CLASSES = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load a (frozen) Tensorflow model into memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow' has no attribute 'GraphDef'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m detection_graph \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mGraph()\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m detection_graph\u001b[38;5;241m.\u001b[39mas_default():\n\u001b[0;32m----> 3\u001b[0m   od_graph_def \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mGraphDef\u001b[49m()\n\u001b[1;32m      4\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mgfile\u001b[38;5;241m.\u001b[39mGFile(PATH_TO_CKPT, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m fid:\n\u001b[1;32m      5\u001b[0m     serialized_graph \u001b[38;5;241m=\u001b[39m fid\u001b[38;5;241m.\u001b[39mread()\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute 'GraphDef'"
     ]
    }
   ],
   "source": [
    "detection_graph = tf.Graph()\n",
    "with detection_graph.as_default():\n",
    "  od_graph_def = tf.GraphDef()\n",
    "  with tf.gfile.GFile(PATH_TO_CKPT, 'rb') as fid:\n",
    "    serialized_graph = fid.read()\n",
    "    od_graph_def.ParseFromString(serialized_graph)\n",
    "    tf.import_graph_def(od_graph_def, name='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading label map\n",
    "Label maps map indices to category names, so that when our convolution network predicts `5`, we know that this corresponds to `airplane`.  Here we use internal utility functions, but anything that returns a dictionary mapping integers to appropriate string labels would be fine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = label_map_util.load_labelmap(PATH_TO_LABELS)\n",
    "categories = label_map_util.convert_label_map_to_categories(label_map, max_num_classes=NUM_CLASSES, use_display_name=True)\n",
    "category_index = label_map_util.create_category_index(categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image_into_numpy_array(image):\n",
    "  (im_width, im_height) = image.size\n",
    "  return np.array(image.getdata()).reshape(\n",
    "      (im_height, im_width, 3)).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the sake of simplicity we will use only 2 images:\n",
    "# image1.jpg\n",
    "# image2.jpg\n",
    "# If you want to test the code with your images, just add path to the images to the TEST_IMAGE_PATHS.\n",
    "PATH_TO_TEST_IMAGES_DIR = 'test_images'\n",
    "TEST_IMAGE_PATHS = [ os.path.join(PATH_TO_TEST_IMAGES_DIR, 'image{}.jpg'.format(i)) for i in range(3, 24) ]\n",
    "\n",
    "# Size, in inches, of the output images.\n",
    "IMAGE_SIZE = (50, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inference_for_single_image(image, graph):\n",
    "  with graph.as_default():\n",
    "    with tf.Session() as sess:\n",
    "      # Get handles to input and output tensors\n",
    "      ops = tf.get_default_graph().get_operations()\n",
    "      all_tensor_names = {output.name for op in ops for output in op.outputs}\n",
    "      tensor_dict = {}\n",
    "      for key in [\n",
    "          'num_detections', 'detection_boxes', 'detection_scores',\n",
    "          'detection_classes', 'detection_masks'\n",
    "      ]:\n",
    "        tensor_name = key + ':0'\n",
    "        if tensor_name in all_tensor_names:\n",
    "          tensor_dict[key] = tf.get_default_graph().get_tensor_by_name(\n",
    "              tensor_name)\n",
    "      if 'detection_masks' in tensor_dict:\n",
    "        # The following processing is only for single image\n",
    "        detection_boxes = tf.squeeze(tensor_dict['detection_boxes'], [0])\n",
    "        detection_masks = tf.squeeze(tensor_dict['detection_masks'], [0])\n",
    "        # Reframe is required to translate mask from box coordinates to image coordinates and fit the image size.\n",
    "        real_num_detection = tf.cast(tensor_dict['num_detections'][0], tf.int32)\n",
    "        detection_boxes = tf.slice(detection_boxes, [0, 0], [real_num_detection, -1])\n",
    "        detection_masks = tf.slice(detection_masks, [0, 0, 0], [real_num_detection, -1, -1])\n",
    "        detection_masks_reframed = utils_ops.reframe_box_masks_to_image_masks(\n",
    "            detection_masks, detection_boxes, image.shape[0], image.shape[1])\n",
    "        detection_masks_reframed = tf.cast(\n",
    "            tf.greater(detection_masks_reframed, 0.5), tf.uint8)\n",
    "        # Follow the convention by adding back the batch dimension\n",
    "        tensor_dict['detection_masks'] = tf.expand_dims(\n",
    "            detection_masks_reframed, 0)\n",
    "      image_tensor = tf.get_default_graph().get_tensor_by_name('image_tensor:0')\n",
    "\n",
    "      # Run inference\n",
    "      output_dict = sess.run(tensor_dict,\n",
    "                             feed_dict={image_tensor: np.expand_dims(image, 0)})\n",
    "\n",
    "      # all outputs are float32 numpy arrays, so convert types as appropriate\n",
    "      output_dict['num_detections'] = int(output_dict['num_detections'][0])\n",
    "      output_dict['detection_classes'] = output_dict[\n",
    "          'detection_classes'][0].astype(np.uint8)\n",
    "      output_dict['detection_boxes'] = output_dict['detection_boxes'][0]\n",
    "      output_dict['detection_scores'] = output_dict['detection_scores'][0]\n",
    "      if 'detection_masks' in output_dict:\n",
    "        output_dict['detection_masks'] = output_dict['detection_masks'][0]\n",
    "  return output_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for image_path in TEST_IMAGE_PATHS:\n",
    "  image = Image.open(image_path)\n",
    "  # the array based representation of the image will be used later in order to prepare the\n",
    "  # result image with boxes and labels on it.\n",
    "  image_np = load_image_into_numpy_array(image)\n",
    "  # Expand dimensions since the model expects images to have shape: [1, None, None, 3]\n",
    "  image_np_expanded = np.expand_dims(image_np, axis=0)\n",
    "  # Actual detection.\n",
    "  output_dict = run_inference_for_single_image(image_np, detection_graph)\n",
    "  # Visualization of the results of a detection.\n",
    "  vis_util.visualize_boxes_and_labels_on_image_array(\n",
    "      image_np,\n",
    "      output_dict['detection_boxes'],\n",
    "      output_dict['detection_classes'],\n",
    "      output_dict['detection_scores'],\n",
    "      category_index,\n",
    "      instance_masks=output_dict.get('detection_masks'),\n",
    "      use_normalized_coordinates=True,\n",
    "      line_thickness=2)\n",
    "  plt.figure(figsize=(16,9))\n",
    "  plt.xticks([])\n",
    "  plt.yticks([])\n",
    "  plt.imshow(image_np)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CONCLUSION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* In this paper, we present the fruit detection and recognition system that we built, using deep convolutional neural network for the recognition of food images from the 8-class dataset that we acquired using Google image searches, while keeping the model training time low to enable faster fine-tuning. Training the model with different epochs further improvement in test accuracy data expansion, which could also be achieved by collecting more training data or by optimizing the architecture and hyper-parameters of the network, considering overtraining problem at the same time. We have written a function which determines calories based on the fruit detected by taking in consideration the average calorie value of that fruit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REFERENCES\n",
    "\n",
    "[1] Y. LeCun, B. Boser, J. S. Denker, D. Henderson, R. E.Howard, W. Hubbard, and L. D. Jackel. Backpropagation applied to handwritten zip code recognition.Neural Comput., 1(4):541–551, Dec. 1989. [2] https://www.kaggle.com/moltean/fruits [3] http://cv-tricks.com/tensorflow-tutorial/training-convolutional-neural-network-for-image-classification/ [4] https://becominghuman.ai/tensorflow-object-detection-api-tutorial-training-and-evaluating-custom-object-detector-ed2594afcf73 [5] https://github.com/tzutalin/labelImg [6] https://github.com/datitran/raccoon_dataset [7] https://towardsdatascience.com/how-to-train-your-own-object-detector-with-tensorflows-object-detector-api-bec72ecfe1d9 [8] http://cv-tricks.com/object-detection/faster-r-cnn-yolo-ssd/ [9] https://faroit.github.io/keras-docs/1.0.1/getting-started/sequential-model-guide/ [10] https://databricks.com/blog/2016/01/25/deep-learning-with-apache-spark-and-tensorflow.html [11] https://towardsdatascience.com/deep-learning-for-object-detection-a-comprehensive-review-73930816d8d9 [12] https://github.com/kawitkars/Food-calorie-estimation-using-deep-learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LICENCES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*The text in the document by Chintan Koticha, Chinmay Keskar, Sneha Kawitkar and is licensed under CC BY 3.0 https://creativecommons.org/licenses/by/3.0/us/\n",
    "\n",
    "*The code in the document by Chintan Koticha, Chinmay Keskar, Sneha Kawitkar and is licensed under the MIT License https://opensource.org/licenses/MIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install object_detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
